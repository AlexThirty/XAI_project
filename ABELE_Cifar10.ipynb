{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4Ef7heHyGPzS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from datetime import datetime as dt\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "random_seed = 42\n",
        "tf.random.set_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AlexThirty/XAI_project.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0C143rnKnto",
        "outputId": "848a9b5b-8391-41fe-dc44-3a5a6dcdda1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'XAI_project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zxlQ41e9GPzU"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "def preprocess_images(images):\n",
        "    images = images.reshape((images.shape[0], 32, 32, 3)).astype('float32') / 255.\n",
        "    return images\n",
        "\n",
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)\n",
        "\n",
        "length = train_images.shape[1]\n",
        "width = train_images.shape[2]\n",
        "channels = train_images.shape[3]\n",
        "\n",
        "train_size = train_images.shape[0]\n",
        "test_size = test_images.shape[0]\n",
        "batch_size = 128\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(train_size).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(test_size).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aB5Ghnh-GPzV"
      },
      "outputs": [],
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=False):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uSJU3TPqGPzV"
      },
      "outputs": [],
      "source": [
        "LR_SCHEDULE = [\n",
        "    (25, 0.0001),\n",
        "    (50, 0.00005),\n",
        "    (75, 0.00002),\n",
        "    (100, 0.00001),\n",
        "]\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
        "        return lr\n",
        "    for i in range(len(LR_SCHEDULE)):\n",
        "        if epoch == LR_SCHEDULE[i][0]:\n",
        "            return LR_SCHEDULE[i][1]\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "COwC-B4tGPzW"
      },
      "outputs": [],
      "source": [
        "class CustomAAECallbacks(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X, schedule, patience=0):\n",
        "        super(CustomAAECallbacks, self).__init__()\n",
        "        # Immagini per la ricostruzione\n",
        "        self.X = X\n",
        "        self.patience = patience\n",
        "        self.schedule = schedule\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.ae_best = np.Inf\n",
        "        self.gen_best = np.Inf\n",
        "        self.dc_best = np.Inf\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        ae_lr = float(tf.keras.backend.get_value(self.model.ae_optimizer.learning_rate))\n",
        "        gen_lr = float(tf.keras.backend.get_value(self.model.gen_optimizer.learning_rate))\n",
        "        dc_lr = float(tf.keras.backend.get_value(self.model.dc_optimizer.learning_rate))\n",
        "        scheduled_ae_lr = self.schedule(epoch, ae_lr)\n",
        "        scheduled_dc_lr = self.schedule(epoch, dc_lr)\n",
        "        scheduled_gen_lr = self.schedule(epoch, gen_lr)\n",
        "        tf.keras.backend.set_value(self.model.ae_optimizer.lr, scheduled_ae_lr)\n",
        "        tf.keras.backend.set_value(self.model.gen_optimizer.lr, scheduled_gen_lr)\n",
        "        tf.keras.backend.set_value(self.model.dc_optimizer.lr, scheduled_dc_lr)\n",
        "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_ae_lr))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save 10 reconstructions every 10 epochs\n",
        "        if epoch%10==0:\n",
        "            self.model.save_10_reconstructions(self.X, epoch)\n",
        "\n",
        "        current_ae_loss = logs.get(\"val_ae_loss\")\n",
        "        current_gen_loss = logs.get(\"val_gen_loss\")\n",
        "        current_dc_loss = logs.get(\"val_dc_loss\")\n",
        "        if np.less(current_ae_loss, self.ae_best) or np.less(current_gen_loss, self.gen_best) or np.less(current_dc_loss, self.dc_best):\n",
        "            self.ae_best = current_ae_loss\n",
        "            self.dc_best = current_dc_loss\n",
        "            self.gen_best = current_gen_loss\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "        \n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DW9nxKaUGPzY"
      },
      "outputs": [],
      "source": [
        "def get_encoder(input_shape, latent_dim, leaky_alpha, filters, kernel_size, strides, dense_units):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=filters[0], kernel_size=kernel_size[0], strides=strides[0], padding='same', kernel_regularizer='l2')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(leaky_alpha)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=filters[1], kernel_size=kernel_size[1], strides=strides[1], padding='same', kernel_regularizer='l2')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(leaky_alpha)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=filters[2], kernel_size=kernel_size[2], strides=strides[2], padding='same', kernel_regularizer='l2')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(leaky_alpha)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=filters[3], kernel_size=kernel_size[3], strides=strides[3], padding='same', kernel_regularizer='l2')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(leaky_alpha)(x)\n",
        "\n",
        "    flatten = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(dense_units[0], activation='relu', kernel_regularizer='l2')(flatten)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
        "    z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
        "\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "    model = tf.keras.Model(inputs, z, name=\"Encoder\")\n",
        "    return model\n",
        "\n",
        "def get_decoder(input_shape, latent_dim, leaky_alpha, filters, kernel_size, strides, dense_units, stride_reduction):\n",
        "    inputs = tf.keras.Input(shape=(latent_dim,))\n",
        "\n",
        "    x = tf.keras.layers.Dense(dense_units[0], activation='relu', kernel_regularizer='l2')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(int(input_shape[0]*input_shape[1]*filters[3]/stride_reduction**2), activation='relu', kernel_regularizer='l2')(x)\n",
        "    \n",
        "    reshaped = tf.keras.layers.Reshape((int(input_shape[0]/stride_reduction), int(input_shape[1]/stride_reduction), filters[3]))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=filters[3], kernel_size=kernel_size[3], strides=strides[3], padding='same', kernel_regularizer='l2')(reshaped)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(leaky_alpha)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=filters[2], kernel_size=kernel_size[2], strides=strides[2], padding='same', kernel_regularizer='l2')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(leaky_alpha)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=filters[1], kernel_size=kernel_size[1], strides=strides[1], padding='same', kernel_regularizer='l2')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(leaky_alpha)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=filters[0], kernel_size=kernel_size[0], strides=strides[0], padding='same', kernel_regularizer='l2')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(leaky_alpha)(x)\n",
        "\n",
        "    image = tf.keras.layers.Conv2DTranspose(filters=input_shape[2], kernel_size=kernel_size[0], strides=1, padding='same', kernel_regularizer='l2')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, image, name=\"Decoder\")\n",
        "    return model\n",
        "\n",
        "def get_discriminator(latent_dim, discriminator_units):\n",
        "    inputs = tf.keras.Input(shape=(latent_dim,))\n",
        "\n",
        "    x = tf.keras.layers.Dense(discriminator_units[0], activation='relu', kernel_regularizer='l2')(inputs)\n",
        "    x = tf.keras.layers.Dense(discriminator_units[1], activation='relu', kernel_regularizer='l2')(x)\n",
        "\n",
        "    vote = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, vote, name=\"Discriminator\")\n",
        "    return model\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AlLGu1HXGPza"
      },
      "outputs": [],
      "source": [
        "class AAE(tf.keras.Model):\n",
        "    def __init__(self, input_shape, latent_dim, leaky_alpha, filters, kernel_size, strides, dense_units, discriminator_units, base_lr, max_lr, step_size, gen_coef, batch_size):\n",
        "        super(AAE, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        # Calculate the stride factor of downsampling\n",
        "        self.stride_reduction = 1\n",
        "        for i, stride in enumerate(strides):\n",
        "            self.stride_reduction = self.stride_reduction * stride\n",
        "        \n",
        "        # Latent dimension\n",
        "        self.latent_dim = latent_dim\n",
        "        # Define losses and accuracies\n",
        "        self.mse = tf.keras.losses.MeanSquaredError()\n",
        "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.dc_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "        # Define the learning rates for cyclic learning rate (not used)\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.gen_coef = gen_coef\n",
        "\n",
        "        # Encoder Net\n",
        "        self.encoder = get_encoder(input_shape, latent_dim, leaky_alpha, filters, kernel_size, strides, dense_units)\n",
        "\n",
        "        # Decoder Net\n",
        "        self.decoder = get_decoder(input_shape, latent_dim, leaky_alpha, filters, kernel_size, strides, dense_units, self.stride_reduction)\n",
        "\n",
        "        # Discriminator Net\n",
        "        self.discriminator = get_discriminator(latent_dim, discriminator_units)\n",
        "\n",
        "    def compile(self, ae_opt, dc_opt, gen_opt):\n",
        "        super(AAE, self).compile()\n",
        "        # Set optimizers\n",
        "        self.ae_optimizer = ae_opt\n",
        "        self.gen_optimizer = gen_opt\n",
        "        self.dc_optimizer = dc_opt\n",
        "        # Set loss functions\n",
        "        self.ae_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "        self.binCe_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        # Set metrics and accuracies\n",
        "        self.dc_acc_fn = tf.keras.metrics.BinaryAccuracy(name='dc_accuracy')\n",
        "        self.ae_metrics = tf.keras.metrics.MeanSquaredError(name='ae_loss')\n",
        "        self.dc_metrics = tf.keras.metrics.BinaryCrossentropy(from_logits=True, name='dc_loss')\n",
        "        self.gen_metrics = tf.keras.metrics.BinaryCrossentropy(from_logits=True, name='gen_loss')\n",
        "        # Compile internal models\n",
        "        self.encoder.compile()\n",
        "        self.decoder.compile()\n",
        "        self.discriminator.compile()\n",
        "\n",
        "    # Define the metrics\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.dc_acc_fn, self.ae_metrics, self.dc_metrics, self.gen_metrics]\n",
        "\n",
        "    # Encoding function\n",
        "    def encode(self, x, training=False):\n",
        "        return self.encoder(x, training=training)\n",
        "\n",
        "    # Decoding function\n",
        "    def decode(self, z, apply_sigmoid=False, training=False):\n",
        "        logits = self.decoder(z, training=training)\n",
        "        if apply_sigmoid:\n",
        "            probs = tf.sigmoid(logits)\n",
        "            return probs\n",
        "        return logits\n",
        "    \n",
        "    # Save 10 reconstructions\n",
        "    def save_10_reconstructions(self, X, epoch):\n",
        "        fig, ax = plt.subplots(2,5,figsize=(15,5))\n",
        "        images = X[:10,:,:,:].reshape((10,32,32,3))\n",
        "        z = self.encode(images)\n",
        "        image_reconstruction = self.decode(z, apply_sigmoid=True).numpy()\n",
        "        for i in range(10):\n",
        "            ax.ravel()[i].imshow(image_reconstruction[i,:])\n",
        "            ax.ravel()[i].axis(False)\n",
        "        fig.savefig(f'reconstructed_epoch_{epoch+1:03d}.png',bbox_inches='tight')\n",
        "\n",
        "    #@tf.function\n",
        "    def test_step(self, batch_x):\n",
        "        generated_noise = tf.random.normal([self.batch_size, self.latent_dim], mean=0.0, stddev=1.0)\n",
        "        z_generated = self.encode(batch_x)\n",
        "        X_reconstructed = self.decode(z_generated, apply_sigmoid=True)\n",
        "                \n",
        "        dc_fake = self.discriminator(z_generated)\n",
        "        dc_real = self.discriminator(generated_noise)\n",
        "\n",
        "        self.ae_metrics.update_state(batch_x, X_reconstructed)\n",
        "        self.dc_metrics.update_state(tf.concat([tf.zeros_like(dc_fake), tf.ones_like(dc_real)], axis=0), tf.concat([dc_fake, dc_real], axis=0))\n",
        "        self.gen_metrics.update_state(tf.ones_like(dc_fake), dc_fake)\n",
        "        self.dc_acc_fn.update_state(tf.concat([tf.zeros_like(dc_fake), tf.ones_like(dc_real)], axis=0), tf.concat([dc_fake, dc_real], axis=0))\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "    # Function for the train step\n",
        "    @tf.function\n",
        "    def train_step(self, batch_x):\n",
        "        # Autoencoder training\n",
        "        with tf.GradientTape() as ae_tape:\n",
        "            z_generated = self.encode(batch_x, training=True)\n",
        "            X_reconstructed = self.decode(z_generated, apply_sigmoid=True, training=True)\n",
        "            ae_loss = self.ae_loss_fn(batch_x, X_reconstructed)\n",
        "\n",
        "        # Apply the gradients\n",
        "        ae_grads = ae_tape.gradient(ae_loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n",
        "        self.ae_optimizer.apply_gradients(zip(ae_grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n",
        "\n",
        "        # Discriminator training with normal prior\n",
        "        generated_noise = tf.random.normal([self.batch_size, self.latent_dim], mean=0.0, stddev=1.0)\n",
        "        with tf.GradientTape() as dc_tape:\n",
        "            encoder_output = self.encode(batch_x, training=False)\n",
        "            dc_fake = self.discriminator(encoder_output, training=True)\n",
        "            dc_real = self.discriminator(generated_noise, training=True)\n",
        "\n",
        "            real_loss = self.binCe_loss_fn(tf.ones_like(dc_real), dc_real)\n",
        "            fake_loss = self.binCe_loss_fn(tf.zeros_like(dc_fake), dc_fake)\n",
        "            dc_loss = real_loss + fake_loss\n",
        "\n",
        "            dc_acc = self.dc_acc_fn(tf.concat([tf.ones_like(dc_real), tf.zeros_like(dc_fake)], axis=0),\n",
        "                        tf.concat([dc_real, dc_fake], axis=0))\n",
        "\n",
        "        # Apply the gradients\n",
        "        dc_grads = dc_tape.gradient(dc_loss, self.discriminator.trainable_variables)\n",
        "        self.dc_optimizer.apply_gradients(zip(dc_grads, self.discriminator.trainable_variables))\n",
        "\n",
        "        # Generator training (Encoder)\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            encoder_output = self.encode(batch_x, training=True)\n",
        "            dc_fake = self.discriminator(encoder_output, training=False)\n",
        "            gen_loss = self.binCe_loss_fn(tf.ones_like(dc_fake),dc_fake)\n",
        "\n",
        "        # Apply the gradients\n",
        "        gen_grads = gen_tape.gradient(gen_loss, self.encoder.trainable_variables)\n",
        "        self.gen_optimizer.apply_gradients(zip(gen_grads, self.encoder.trainable_variables))\n",
        "\n",
        "        # Update the metrics\n",
        "        self.ae_metrics.update_state(batch_x, X_reconstructed)\n",
        "        self.dc_metrics.update_state(tf.concat([tf.zeros_like(dc_fake), tf.ones_like(dc_real)], axis=0), tf.concat([dc_fake, dc_real], axis=0))\n",
        "        self.gen_metrics.update_state(tf.ones_like(dc_fake), dc_fake)\n",
        "        self.dc_acc_fn.update_state(tf.concat([tf.zeros_like(dc_fake), tf.ones_like(dc_real)], axis=0), tf.concat([dc_fake, dc_real], axis=0))\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gOthn1YxGPzf"
      },
      "outputs": [],
      "source": [
        "base_lr = 0.0001\n",
        "max_lr = 0.0025\n",
        "step_size = 2 * np.ceil(train_images.shape[0] / batch_size)\n",
        "epochs = 150\n",
        "\n",
        "latent_dim = 256\n",
        "alpha_leaky = 0.2\n",
        "filters = [64,128,256,512]\n",
        "kernel_size = [4,4,3,3]\n",
        "strides = [2,2,2,2]\n",
        "dense_units = [1000,300]\n",
        "discriminator_units = [200, 200]\n",
        "keep_prob = 0.5\n",
        "gen_coef = 1.5\n",
        "\n",
        "steps_per_epoch = train_images.shape[0] / batch_size\n",
        "\n",
        "aae = AAE((length, width, channels), latent_dim, alpha_leaky, filters, kernel_size, strides, dense_units, discriminator_units, base_lr, max_lr, step_size, gen_coef, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "tu-Uxa9EGPzi",
        "outputId": "bb73c91b-8fa7-4e8e-d8b2-b56f87168c6b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-486e9373d058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/XAI_project/enc_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/XAI_project/dec_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/XAI_project/dc_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     return CheckpointReader.CheckpointReader_GetTensor(\n\u001b[0;32m---> 67\u001b[0;31m         self, compat.as_bytes(tensor_str))\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Read less bytes than requested"
          ]
        }
      ],
      "source": [
        "aae.encoder = tf.keras.models.load_model('/content/XAI_project/enc_model')\n",
        "aae.decoder = tf.keras.models.load_model('/content/XAI_project/dec_model')\n",
        "aae.discriminator = tf.keras.models.load_model('/content/XAI_project/dc_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9_nBWSrGPzi"
      },
      "outputs": [],
      "source": [
        "filters = [32, 64, 128]\n",
        "strides = [1, 1, 1]\n",
        "kernel_size = [3, 3, 3]\n",
        "dense_dim = [100, 50]\n",
        "input_shape = (32,32,3)\n",
        "num_labels = 10\n",
        "\n",
        "CNN = tf.keras.Sequential(\n",
        "    [\n",
        "    tf.keras.layers.InputLayer(input_shape=input_shape),\n",
        "    tf.keras.layers.Conv2D(filters=filters[0], kernel_size=kernel_size[0], strides=strides[0], activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Conv2D(filters=filters[1], kernel_size=kernel_size[1], strides=strides[1], activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Conv2D(filters=filters[2], kernel_size=kernel_size[2], strides=strides[2], activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(dense_dim[0], activation='relu'),\n",
        "    tf.keras.layers.Dense(dense_dim[1], activation='relu'),\n",
        "    tf.keras.layers.Dense(num_labels),\n",
        "    ]\n",
        ")\n",
        "\n",
        "CNN.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi8iKXgjGPzi"
      },
      "outputs": [],
      "source": [
        "CNN = tf.keras.models.load_model('/content/drive/MyDrive/Blackboxes/CNN_black_box')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/francescanaretto/XAI-course_2021.git"
      ],
      "metadata": {
        "id": "grJOPpBfjCsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd XAI-course_2021/Images/"
      ],
      "metadata": {
        "id": "VG-97IRvn53t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "LdvOY91roSi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deap"
      ],
      "metadata": {
        "id": "L62Zgl5Ko0Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq0t0s7_GPzj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()\n",
        "\n",
        "import time\n",
        "from skimage.color import gray2rgb, rgb2gray\n",
        "from skimage import feature, transform\n",
        "#disable eager execution in tensorflow 2.x for faster training time\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()\n",
        "\n",
        "from externals.ABELE.ilore.ilorem import ILOREM\n",
        "from externals.ABELE.ilore.util import neuclidean\n",
        "\n",
        "from externals.ABELE.experiments.exputil import get_dataset\n",
        "from externals.ABELE.experiments.exputil import get_autoencoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "random_state = 42\n",
        "dataset = 'custom' \n",
        "black_box = 'AB' #agnostic Black Box\n",
        "\n",
        "# load autoencoder and black box\n",
        "ae_name = 'aae' \n",
        "path = './' \n",
        "path_aemodels = path + 'models/abele/%s/%s/' % (dataset, ae_name)\n",
        "bb = tf.keras.models.load_model(\"/content/drive/MyDrive/Blackboxes/CNN_black_box\")\n",
        "# defining a functions for bb to return the class index value\n",
        "def bb_predict(X):\n",
        "    X = X.astype(float)\n",
        "    Y = bb.predict(X)   \n",
        "    return np.argmax(Y, axis=1)\n",
        "\n",
        "# load data\n",
        "dataset = 'custom'\n",
        "use_rgb=True\n",
        "\n",
        "# load auto encoder\n",
        "\n",
        "class_name = 'class'\n",
        "class_values = ['%s' % i for i in range(len(np.unique(test_labels)))]\n",
        "\n",
        "# index Image 2 Explain\n",
        "img = test_images[2]\n",
        "# time\n",
        "start = time.time()\n",
        "# create explainer\n",
        "\"\"\"\n",
        "Arguments:\n",
        "    bb_predict: function which return the prediction of the blackbox in form index of the class \n",
        "    class_name: name of the class used when printing rules (class_name: class_value)\n",
        "    class_values: list of names of the classes (class_name: class_value)\n",
        "    neigh_type: select the nighbourhood type,\n",
        "                supportecd types:\n",
        "                'gnt' : genetic\n",
        "                'rnd' : random\n",
        "                'hrg' : hybrid-random-genetic\n",
        "                'gntp': genetic probabilistic\n",
        "                'hrgp': hybrid probabilistic\n",
        "    ocr: [0.1] other class values, ratio of other class from the one predicted in the neighbourhood\n",
        "    kernel: [None] Kernel to weights the point in the nieghbourhood\n",
        "    kernel_width : [None]  \n",
        "    autoencoder: Autoencoder to generate the latent space points\n",
        "    use_rgb = [True] Set to True if the input images are rgb, False for grayscale\n",
        "    filter_crules: [None] if True Prototypes are checked by the black box to be the same class of the query image\n",
        "    random_state: set the seed of the random state\n",
        "    verbose: True if you want to print more informations\n",
        "    NEIGHBOURHOOD PARAMETERS: the following parameters are Neighbourhood specific and may not apply to all of the neighbourhood types\n",
        "        valid_thr: [0.5] threshold to change class in the autoencoder disciminator\n",
        "        alpha1: [0.5] weight of the feature similarity of the neighbourhood points\n",
        "        alpha2: [0.5] weight of the target similarity of the neighbourhood points\n",
        "        ngen: [100] number of generations of the genetic algorithm\n",
        "        mutpb: [0.2] The probability of mutating an individual in the genetic algorithm\n",
        "        cxpb: [0.5] The probability of mating two individuals in the genetic algorithm\n",
        "        tournsize: [3] number of tournaments in the genetic algorithm\n",
        "        hallooffame_ratio: [0.1] Fraction of exemplars to keep at every genetic generation\n",
        "\"\"\"\n",
        "explainer = ILOREM(bb_predict, \n",
        "                   class_name, \n",
        "                   class_values, \n",
        "                   neigh_type='hrg',\n",
        "                   ocr=0.1,\n",
        "                   kernel_width=None, \n",
        "                   kernel=None, \n",
        "                   autoencoder=aae, \n",
        "                   use_rgb=use_rgb, \n",
        "                   filter_crules=True, \n",
        "                   random_state=random_state, \n",
        "                   verbose=True, \n",
        "                   valid_thr=0.5,\n",
        "                   alpha1=0.5, \n",
        "                   alpha2=0.5,\n",
        "                   metric=neuclidean, \n",
        "                   ngen=100, \n",
        "                   mutpb=0.2, \n",
        "                   cxpb=0.5, \n",
        "                   tournsize=3, \n",
        "                   halloffame_ratio=0.1)\n",
        "\n",
        "\"\"\"\n",
        "generate an explanation for a given image\n",
        "Arguments:\n",
        "    img: the image to explain\n",
        "    num_samples: [1000] number of samples to generate with the neighbourhood algorithm\n",
        "    use_weights: [True] if weights the points using distance\n",
        "Return:\n",
        "Explanation object compose by several things\n",
        "    rstr: string describing the rule\n",
        "    cstr: string describing the counterfactual rule\n",
        "    bb_pred: black box prediction of the image\n",
        "    dt_pred: decisoon tree prediction\n",
        "    fidelity: fidelity between black box and the decision tree\n",
        "    limg: latent space representation of the image\n",
        "\"\"\"\n",
        "exp = explainer.explain_instance(img, num_samples=1000, use_weights=True, metric=neuclidean)\n",
        "# time\n",
        "end = time.time()\n",
        "print('--------------------------')\n",
        "print('execution time: ',end - start,' sec')\n",
        "print('e = {\\n\\tr = %s\\n\\tc = %s    \\n}' % (exp.rstr(), exp.cstr()))\n",
        "print('--------------------------')\n",
        "print('bb prediction of reconstructed image: ',exp.bb_pred,'dt prediction: ',exp.dt_pred,'fidelity: ',exp.fidelity)\n",
        "print('latent space representation: ',exp.limg)\n",
        "\n",
        "\"\"\"\n",
        "Arguments:\n",
        "    features: [None] list of which feature of the latent space to use, If None use all\n",
        "    samples: [10] number of prototype to use\n",
        "Return the image and the difference between the prototypes\n",
        "\"\"\"\n",
        "img2show, mask = exp.get_image_rule(features=None, samples=400)\n",
        "\n",
        "# Plot Script\n",
        "F, ax = plt.subplots(1,2, figsize=(10,5))\n",
        "if use_rgb:\n",
        "    ax[0].imshow(img2show)\n",
        "else:\n",
        "    ax[0].imshow(img2show, cmap='gray')\n",
        "bbo = bb_predict(np.array([img2show]))[0]\n",
        "ax[0].set_title('Image to explain - black box %s' % bbo)\n",
        "ax[0].axis('off')\n",
        "dx, dy = 0.05, 0.05\n",
        "xx = np.arange(0.0, img2show.shape[1], dx)\n",
        "yy = np.arange(0.0, img2show.shape[0], dy)\n",
        "xmin, xmax, ymin, ymax = np.amin(xx), np.amax(xx), np.amin(yy), np.amax(yy)\n",
        "extent = xmin, xmax, ymin, ymax\n",
        "cmap_xi = plt.get_cmap('Greys_r')\n",
        "cmap_xi.set_bad(alpha=0)\n",
        "# Compute edges (to overlay to heatmaps later)\n",
        "percentile = 100\n",
        "dilation = 3.0\n",
        "alpha = 0.8\n",
        "xi_greyscale = img2show if len(img2show.shape) == 2 else np.mean(img2show, axis=-1)\n",
        "in_image_upscaled = transform.rescale(xi_greyscale, dilation, mode='constant')\n",
        "edges = feature.canny(in_image_upscaled).astype(float)\n",
        "edges[edges < 0.5] = np.nan\n",
        "edges[:5, :] = np.nan\n",
        "edges[-5:, :] = np.nan\n",
        "edges[:, :5] = np.nan\n",
        "edges[:, -5:] = np.nan\n",
        "overlay = edges\n",
        "ax[1].imshow(mask, extent=extent, cmap=plt.cm.BrBG, alpha=1, vmin=0, vmax=255)\n",
        "ax[1].imshow(overlay, extent=extent, interpolation='none', cmap=cmap_xi, alpha=alpha)\n",
        "ax[1].axis('off')\n",
        "ax[1].set_title('Attention area respecting latent rule');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db_JnzS8GPzj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Return the prototypes images\n",
        "Arguments:\n",
        "    num_prototypes: [5] number of prototypes to return\n",
        "    return_latent: [False] if True return latent representation\n",
        "    return_diff: [False] If True return the difference with the query image\n",
        "    features: [None] list of the features int he latent space to use, if none use all\n",
        "\"\"\"\n",
        "proto = exp.get_prototypes_respecting_rule(num_prototypes=5)\n",
        "\n",
        "F, ax = plt.subplots(1,5,figsize=(30,5))\n",
        "for i in range(5):\n",
        "    ax[i].imshow(proto[i])\n",
        "    ax[i].axis('off')\n",
        "    ax[i].set_title('model prediction: '+str(bb_predict(rgb2gray(proto[0])[np.newaxis,:,:,np.newaxis])[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7yhjVoDGPzk"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Return the couterfactuals satisfying the counterfactual rule\n",
        "\"\"\"\n",
        "counter = exp.get_counterfactual_prototypes()\n",
        "\n",
        "F, ax = plt.subplots(1,len(counter),figsize=(30,5))\n",
        "if len(counter)==0:\n",
        "    print('no counterfactual found')\n",
        "elif len(counter)==1:\n",
        "    plt.imshow(counter[0]/255)\n",
        "    plt.axis('off')\n",
        "    plt.set_title('model prediction: '+str(bb_predict(counter[0])[0]))    \n",
        "for i in range(len(counter)):\n",
        "    ax[i].imshow(counter[i]/255)\n",
        "    ax[i].axis('off')\n",
        "    ax[i].set_title('model prediction: '+str(bb_predict(counter[i])[0]))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "39ffb1c44105ac059136d352d026159bcca195c13c9355765ec3c2c6a6f716ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "ABELE_Cifar10.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
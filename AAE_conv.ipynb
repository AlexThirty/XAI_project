{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAE:\n",
    "  def __init__(self, latent_dim, input_shape, input_length, kernel_size, strides, filters, discriminator_dims, keep_prob, ae_lr, dc_lr, gen_lr):\n",
    "    super(AAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.ae_optimizer = tf.keras.optimizers.Adam(learning_rate=ae_lr)\n",
    "    self.gen_optimizer = tf.keras.optimizers.Adam(learning_rate=gen_lr)\n",
    "    self.dc_optimizer = tf.keras.optimizers.Adam(learning_rate=dc_lr)\n",
    "    self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    self.dc_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "    self.stride_reduction = strides[0]*strides[1]\n",
    "\n",
    "    self.encoder = tf.keras.Sequential(\n",
    "        [\n",
    "          tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "          tf.keras.layers.Conv2D(filters=filters[0], kernel_size=kernel_size[0], strides=strides[0], activation='relu'),\n",
    "          tf.keras.layers.Conv2D(filters=filters[1], kernel_size=kernel_size[1], strides=strides[1], activation='relu'),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          # Obtain mu e log_var\n",
    "          tf.keras.layers.Dense(latent_dim + latent_dim)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.decoder = tf.keras.Sequential(\n",
    "        [\n",
    "          tf.keras.layers.InputLayer(input_shape=latent_dim),\n",
    "          tf.keras.layers.Dense(int(input_shape[0]*input_shape[1]*32/self.stride_reduction**2), activation='relu'),\n",
    "          tf.keras.layers.Reshape(target_shape=(int(input_shape[0]/self.stride_reduction), int(input_shape[1]/self.stride_reduction), 32)),\n",
    "          tf.keras.layers.Conv2DTranspose(filters=filters[1], kernel_size=kernel_size[1], strides=strides[1], activation='relu', padding='same'),\n",
    "          tf.keras.layers.Conv2DTranspose(filters=filters[0], kernel_size=kernel_size[0], strides=strides[0], activation='relu', padding='same'),\n",
    "          tf.keras.layers.Conv2DTranspose(filters=input_shape[2], kernel_size=kernel_size[0], strides=1, padding='same'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.discriminator = tf.keras.Sequential(\n",
    "        [\n",
    "          tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "          tf.keras.layers.Dense(discriminator_dims[0], activation='relu'),\n",
    "          tf.keras.layers.Dropout(keep_prob),\n",
    "          tf.keras.layers.Dense(discriminator_dims[1], activation='relu'),\n",
    "          tf.keras.layers.Dropout(keep_prob),\n",
    "          tf.keras.layers.Dense(1),\n",
    "        ]\n",
    "    )\n",
    "  \n",
    "  def sample(self, n_samples, eps=None):\n",
    "    if eps is None:\n",
    "      eps = tf.random.normal(shape=(n_samples, self.latent_dim))\n",
    "    return self.decode(eps, apply_sigmoid=True)\n",
    "  \n",
    "  def encode(self, x, training=False):\n",
    "    mean, log_var = tf.split(self.encoder(x, training=training), num_or_size_splits=2, axis=1)\n",
    "    return mean, log_var\n",
    "\n",
    "  def reparametrize(self, mean, log_var):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(log_var * .5) + mean\n",
    "  \n",
    "  def encode_and_reparametrize(self, x, training=False):\n",
    "    mean, log_var = tf.split(self.encoder(x, training=training), num_or_size_splits=2, axis=1)\n",
    "    return self.reparametrize(mean, log_var)\n",
    "\n",
    "  def decode(self, z, apply_sigmoid=False, training=False):\n",
    "    logits = self.decoder(z, training=training)\n",
    "    if apply_sigmoid:\n",
    "      probs = tf.sigmoid(logits)\n",
    "      return probs\n",
    "    return logits\n",
    "\n",
    "  def log_normal_pdf(sample, mean, log_var, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-log_var) + log_var + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "  def autoencoder_loss(self, X, training=False):\n",
    "    # Generate latent space vector by encoding X\n",
    "    mean_generated, log_var_generated = self.encode(X, training=training)\n",
    "    z_generated = self.reparametrize(mean_generated, log_var_generated)\n",
    "\n",
    "    # Decode\n",
    "    X_generated = self.decode(z_generated, apply_sigmoid=True, training=training)\n",
    "\n",
    "    # Autoencoder loss\n",
    "    ae_loss = tf.reduce_mean(tf.math.squared_difference(X_generated, X))\n",
    "    return ae_loss\n",
    "    \n",
    "\n",
    "  def generator_loss(self, real_output, training=False):\n",
    "    return self.cross_entropy(tf.ones_like(real_output), real_output)\n",
    "\n",
    "  def discriminator_loss(self, real_output, fake_output, training=False):\n",
    "    real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "  \n",
    "  @tf.function\n",
    "  def train_step(self, batch_x):\n",
    "    # Autoencoder\n",
    "    with tf.GradientTape() as ae_tape:\n",
    "        encoder_output = self.encode_and_reparametrize(batch_x, training=True)\n",
    "        decoder_output = self.decode(encoder_output, apply_sigmoid=True, training=True)\n",
    "\n",
    "        # Autoencoder loss\n",
    "        ae_loss = self.autoencoder_loss(batch_x, training=True)\n",
    "    \n",
    "    ae_grads = ae_tape.gradient(ae_loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n",
    "    self.ae_optimizer.apply_gradients(zip(ae_grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n",
    "\n",
    "    generated_noise = tf.random.normal([batch_x.shape[0], self.latent_dim], mean=0.0, stddev=1.0)\n",
    "    # Discriminator\n",
    "    with tf.GradientTape() as dc_tape:\n",
    "        encoder_output = self.encode_and_reparametrize(batch_x, training=True)\n",
    "\n",
    "        dc_real = self.discriminator(encoder_output, training=True)\n",
    "        dc_fake = self.discriminator(generated_noise, training=True)\n",
    "\n",
    "        # Discriminator Loss\n",
    "        dc_loss = self.discriminator_loss(dc_real, dc_fake, training=True)\n",
    "        \n",
    "        # Discriminator Acc\n",
    "        dc_acc = self.dc_accuracy(tf.concat([tf.ones_like(dc_real), tf.zeros_like(dc_fake)], axis=0),\n",
    "                          tf.concat([dc_real, dc_fake], axis=0))\n",
    "\n",
    "    dc_grads = dc_tape.gradient(dc_loss, self.discriminator.trainable_variables)\n",
    "    self.dc_optimizer.apply_gradients(zip(dc_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "    # Generator (Encoder)\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        encoder_output = self.encode_and_reparametrize(batch_x, training=True)\n",
    "        dc_fake = self.discriminator(encoder_output, training=True)\n",
    "\n",
    "        # Generator loss\n",
    "        gen_loss = self.generator_loss(dc_fake, training=True)\n",
    "\n",
    "    gen_grads = gen_tape.gradient(gen_loss, self.encoder.trainable_variables)\n",
    "    self.gen_optimizer.apply_gradients(zip(gen_grads, self.encoder.trainable_variables))\n",
    "\n",
    "    return ae_loss, dc_loss, dc_acc, gen_loss\n",
    "  \n",
    "  def fit(self, train_dataset, epochs):\n",
    "    past = dt.now()\n",
    "    for epoch in range(epochs):\n",
    "      epoch_start = dt.now()\n",
    "      for train_x in train_dataset:\n",
    "        ae_loss, dc_loss, dc_acc, gen_loss = self.train_step(train_x)\n",
    "      epoch_end = dt.now()\n",
    "      print(\"Epoch: {}, time elapsed: {}, AE loss: {}, Discriminator loss: {}, Discriminator accuracy: {}, Generator loss: {}\".format(epoch + 1, epoch_end - epoch_start, ae_loss, dc_loss, dc_acc, gen_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, time elapsed: 0:00:45.960150, AE loss: 0.04000985622406006, Discriminator loss: 0.006326778791844845, Discriminator accuracy: 0.9444800019264221, Generator loss: 1.8000533600570634e-05\n",
      "Epoch: 2, time elapsed: 0:00:32.413406, AE loss: 0.022989574819803238, Discriminator loss: 0.005093975458294153, Discriminator accuracy: 0.9721800088882446, Generator loss: 8.790342320708078e-08\n",
      "Epoch: 3, time elapsed: 0:00:32.740861, AE loss: 0.021613363176584244, Discriminator loss: 0.0012938177678734064, Discriminator accuracy: 0.9814366698265076, Generator loss: 3.4386059155622206e-07\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "def preprocess_images(images):\n",
    "    images = images.reshape((images.shape[0], 32, 32, 3)).astype('float32') /255.\n",
    "    return images\n",
    "\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "\n",
    "train_size = train_images.shape[0]\n",
    "test_size = test_images.shape[0]\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(train_size).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(test_size).batch(batch_size)\n",
    "\n",
    "aae = AAE(40, (32,32,3), 32*32*3, [3,3], [2,2], [32, 64], [400, 150], 0.5, 1e-4, 1e-4, 1e-4)\n",
    "aae.fit(train_dataset, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "802d4647a7a0386ab071d09b3b926d986a71258cf56021a9fa597b7a13f049b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
